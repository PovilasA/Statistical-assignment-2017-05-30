# setting seed for reproducibility.
set.seed(1)
# Indices for training set and test set. We take 70% of the data for training.
train_indices = sample(1:length(model_data1[,1]), length(model_data1[,1])*0.7)
test_indices = setdiff(1:length(model_data1[,1]), train_indices)

train = model_data1[train_indices,]
test = model_data1[test_indices,]

# Run procedure to select variables for modeling. This creates variables
# 'selected_variables' (vector of selected variables) and 
# 'formula'(formula with selected features used for modeling).
source('boruta.R')


# Logistic regression
model <- glm(formula, family=binomial(link='logit'), data=train)

# we run the anova() to analyze the table of deviance.
anova(model, test="Chisq")
# The difference between the null deviance and the residual deviance shows how
# our model is doing against the null model (a model with only the intercept).
# The wider this gap, the better.


fitted.results <- predict(model, newdata=test[selected_features], type='response')

print_accuracy_for_different_cutoff <- function(results, target, n=20) {
  for (cutoff in 1:(n-1)/n) {
    fitted.results.binary <- ifelse(results > cutoff,1,0)
    misClasificError <- mean(fitted.results.binary != target)
    print(paste(cutoff, 'Accuracy', 1-misClasificError))
  } 
}

# Trying different cutoff probabilities
print_accuracy_for_different_cutoff(fitted.results, test$Target)
# We can see that 0.5 cutoff gives maximum accuracy which is 0.69


# The ROC is a curve generated by plotting the true positive rate (TPR) against
# the false positive rate (FPR) at various threshold settings.
library(ROCR)
p <- predict(model, newdata=test[selected_features], type="response")
pr <- prediction(p, test$Target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

# AUC is the area under the ROC curve. 
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
# AUC is equal to 0.67

# Little analysis of prediction data (data which will be used for prediction).
missmap(pred_data[selected_features], main = "Missing values vs observed")
# All selected features have no missing value in prediction dataset.



## Ridge Regression
library(glmnet)
set.seed(10)

# glmnet only accepts matrices as a data input.
xfactors <- model.matrix(formula, data = train)[, -1]
x <- as.matrix(data.frame(xfactors))

# cv.glmnet makes k-fold cross-validation for glmnet, produces a plot, and 
# results for different lambdas
cv.ridge <- cv.glmnet(x, train[["Target"]], family="binomial")

# Results
plot(cv.ridge)

# value of lambda that gives minimum mean cross-validated error.
cv.ridge$lambda.min
# Equals to 0.007. Since it is close to zero we may guess that it will not make
# big impact to prediction.

xfactors <- model.matrix(formula, data = test)[, -1]
newx <- as.matrix(data.frame(xfactors))
fitted.results = predict(cv.ridge, s=cv.ridge$lambda.min, newx, type="response")

# Trying different cutoff probabilities
print_accuracy_for_different_cutoff(fitted.results, test$Target)
# We can see that 0.5 cutoff gives maximum accuracy which is 0.71.
# It is only slightly better performance than simple Logistic regression.


