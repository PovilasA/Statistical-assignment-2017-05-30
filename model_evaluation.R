# Trying different cutoff probabilities
print_accuracy_for_different_cutoff <- function(results, target, n=20) {
  for (cutoff in 1:(n-1)/n) {
    fitted.results.binary <- ifelse(results > cutoff,1,0)
    misClasificError <- mean(fitted.results.binary != target)
    print(paste(cutoff, 'Accuracy', 1-misClasificError))
  } 
}

# The ROC is a curve generated by plotting the true positive rate (TPR) against
# the false positive rate (FPR) at various threshold settings.
ROC <- function(model, test_data){
  p <- predict(model, newdata=test_data[selected_features], type="response")
  pr <- prediction(p, test_data$Target)
  prf <- performance(pr, measure = "tpr", x.measure = "fpr")
  plot(prf)
  
  # AUC is the area under the ROC curve. 
  auc <- performance(pr, measure = "auc")
  auc <- auc@y.values[[1]]
  auc
}
